# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.11.4
#   kernelspec:
#     display_name: Python 3 (PHYS-581-2021)
#     language: python
#     metadata:
#       debugger: true
#     name: phys-581-2021
#     resource_dir: /home/user/.local/share/jupyter/kernels/phys-581-2021
# ---

import mmf_setup;mmf_setup.nbinit()

# # Bayesian Analysis

# Here we work through the example of fitting data with a sine wave.
#
# Let us assume that some data is being generated by the following function:
#
# $$
#   y(t) = f(t) = A\cos(\omega t + \phi).
# $$
#
# The data is $D = \{(t_0, y_0), (t_1, y_1), \dots, (t_{N-1}, y_{N-1})\}$:
#
# $$
#   y_i = f_{\vect{a}}(t_i) + e_i
# $$
#
# where $e_i$ is some random noise with distribution $P_e(e)$ and our model parameters are $\vect{a} = (A, \omega, \phi)$.
#
# To apply Bayes' theorem, we need prior information $P(\vect{a}|I)$ on these parameters, which we can then update based on the data:
#
# $$
#   P(\vect{a}|D,I) = \frac{P(D|\vect{a}, I)P(\vect{a}|I)}{P(D|I)}.
# $$
#
# We need to compute the likelihood $P(D|\vect{a}, I)$ of realizing the data $D$ given the underlying parameters $\vect{a}$ and prior information $I$.  The denominator can then be computed as a normalization factor.

# To estimate the likelihood, we will need to interpret the *noise* distribution $P_i(e)$ instead as a characterization of our measurement process.  How likely are we to measure $(t_i, y_i)$ if the parameters are $\vect{a}$?  In general, this will be some distribution $P_i(t_i, y_i, \vect{a})$:
#
# $$
#   P(D|\vect{a}, I) = \prod_{i} P_i\big(t_i, y_i, \vect{a}\big).
# $$
#
# In our cases, however, we generally consider it certain that we can take measurements at a prescribed set of times $t_i$, and, as discussed above, we assume that the measurement will be affected by random noise $e_i$ with distribution $P_e(e)$, thus, we have:
#
# $$
#   P_i\big(t_i, y_i, \vect{a}\big) = P_e\big(y_i - f_{\vect{a}}(t_i)\big)\\
#   P(D|\vect{a}, I) = \prod_{i} P_e\big(y_i - f_{\vect{a}}(t_i)\big)
# $$
#
# *Note: the priori information $I$ may not seem to enter into the formula, but is where the information about the measurement process comes in.  In this case, it informs us about the distribution of errors $P_i(e)$.*

# ## Neutron Stars

# Consider the problems of constraining parameters $\vect{a}$ that describe the equation of state of a neutron star.  Solving the TOV equations gives a function $M(\vect{a}, p_c) \in \big(1, M_{\max}(\vect{a})\big)$ where $p_c$ is the central pressure.  Given some prior distribution $P(\vect{a}|I)$ on the parameters $\vect{a}$, what can we learn from some observation $D$ of a neutron star described by a distribution $P_e(m)$ on the measured mass?  As before:
#
# $$
#   P(\vect{a}|D,I) = \frac{P(D|\vect{a}, I)P(\vect{a}|I)}{P(D|I)}
# $$
#
# where we must compute the likelihood $P(D|\vect{a}, I)$ of the observation.
#
#
# Let's suppose that we can come up with some prior on $p_c$ 
#
#
#
# The neutron star equation of state says that
#
#
#

# Let's now apply this and see what we what we can learn from some data.
